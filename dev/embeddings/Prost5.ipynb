{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e53d479",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "# Generar embeddigs de estructura utilizando Prost5\n",
    "\n",
    "Para generar embeddigs de estructuras de proteínas con Prost5, priemro es indisponsable preprocesar las estructuras para convertirlas en formato 3Di. Esto se puede hacer utilizando Foldseek, proceso se realizó en el notebook `dev/embeddings/Foldseek_3di.py` de este repocitorio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489f6550",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar libreias necesarias\n",
    "import h5py\n",
    "import numpy as np\n",
    "import os\n",
    "from itertools import islice\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8580cd62",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'ProstT5'...\n"
     ]
    }
   ],
   "source": [
    "# Clonar el repositorio de Prost5\n",
    "!git clone https://github.com/mheinzinger/ProstT5.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71924b14-2dac-43e5-8e5d-a08203d2c97e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  ProstT5-main.zip\n",
      "d9858ad5eb774d5bc7ca5dc31d8d364049ccc87b\n",
      "   creating: ProstT5-main/\n",
      "  inflating: ProstT5-main/LICENSE    \n",
      "  inflating: ProstT5-main/README.md  \n",
      "   creating: ProstT5-main/cnn_chkpnt/\n",
      "  inflating: ProstT5-main/cnn_chkpnt/README.md  \n",
      "  inflating: ProstT5-main/cnn_chkpnt/model.pt  \n",
      "   creating: ProstT5-main/cnn_chkpnt_AA_CNN/\n",
      "  inflating: ProstT5-main/cnn_chkpnt_AA_CNN/README.md  \n",
      "  inflating: ProstT5-main/cnn_chkpnt_AA_CNN/model.pt  \n",
      "   creating: ProstT5-main/notebooks/\n",
      "  inflating: ProstT5-main/notebooks/ProstT5_inverseFolding.ipynb  \n",
      "  inflating: ProstT5-main/prostt5_sketch2.png  \n",
      "   creating: ProstT5-main/scripts/\n",
      "  inflating: ProstT5-main/scripts/README.md  \n",
      "  inflating: ProstT5-main/scripts/embed.py  \n",
      "  inflating: ProstT5-main/scripts/finetune_prostt5_lora_script.py  \n",
      "  inflating: ProstT5-main/scripts/generate_foldseek_db.py  \n",
      "  inflating: ProstT5-main/scripts/predict_3Di_encoderOnly.py  \n",
      "  inflating: ProstT5-main/scripts/predict_AA_encoderOnly.py  \n",
      "   creating: ProstT5-main/scripts/pretraining_scripts/\n",
      "  inflating: ProstT5-main/scripts/pretraining_scripts/pretraining_stage1_MLM.py  \n",
      "  inflating: ProstT5-main/scripts/pretraining_scripts/pretraining_stage2_translation.py  \n",
      "  inflating: ProstT5-main/scripts/translate.py  \n"
     ]
    }
   ],
   "source": [
    "# Descomprimir\n",
    "! unzip ProstT5-main.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda78175",
   "metadata": {},
   "source": [
    "## Generación de los embeddings\n",
    "\n",
    "En este apartado, se utilizaron los archivos 3Di de las estructuras de las proteínas. Estos se encuentran en este repositorio en la ruta `data/embeddings/estructura/3Dmi`\n",
    "Los archivos utilizados son los siguientes: \n",
    "* Datos de entrenamiento: train_3di.fasta \n",
    "* Datos de validación: val_3di.fasta\n",
    "* Datos de evaluación: test_3di.fasta\n",
    "\n",
    "Para generar los embeddings se corrio el comando:\n",
    "\n",
    "`! python ProstT5-main/scripts/embed.py --input ruta/datos.fasta --output ProstT5_output/structure_embeddings_datos.h5 --half 1 --is_3Di 1 --per_protein 1`\n",
    "\n",
    "Este permite generar embeddings por proteína realizando un promedio de los embeddings de los residuos por proteína.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "601ff4b1",
   "metadata": {},
   "source": [
    "### Conjunto de datos de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a33e2739-3123-49fc-a14f-6f886a64fb24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-04 13:05:36.736165: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-05-04 13:05:36.758789: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-05-04 13:05:37.089460: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Using device: cuda:0\n",
      "Loading T5 from: Rostlab/ProstT5\n",
      "/home/sis.virtual.uniandes.edu.co/as.villa/miniconda3/envs/bioembed/lib/python3.8/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Using model in half-precision!\n",
      "########################################\n",
      "Input is 3Di: True\n",
      "Example sequence: A0A023PXQ4\n",
      "ddpdpppvvvvvvvvvvvvvvvvvvvvvvvcvvpvpvvvvvvccvvppppppddppppvpvvvpppppdddddddddaddaddaddddadddddyddhldhdddddddpdddddlpnvlnvvvvvvvppddddddddpdppvsvvvnvlvvvvvvvvvvvvvvvvvvpppdddddpppvvvvvvvpdddddddddddddaddaddddpdcpvvvvvvvvvvvsppddhlydhddddddddddpddpvvcvvvvvrdddppdddddddddddpvvpvvvvppddddddddddddddddppddddddddddddddddddddddddddddddddddpdppvvvvvvvvvvvvvvvvvvvvvvlvvvvvvddpvvnvvsvvvvvvvvcvvvvppppdd\n",
      "########################################\n",
      "Total number of sequences: 18256\n",
      "Average sequence length: 465.8905565293602\n",
      "Number of sequences >1000: 1312\n",
      "Example: embedded protein Q95Q95 with length 2697 to emb. of shape: torch.Size([1024])\n",
      "\n",
      "############# STATS #############\n",
      "Total number of embeddings: 18256\n",
      "Total time: 439.50[s]; time/prot: 0.0241[s]; avg. len= 465.89\n"
     ]
    }
   ],
   "source": [
    "!python ProstT5-main/scripts/embed.py --input 3di_data/train_3di.fasta --output ProstT5_output/structure_embeddings_train.h5 --half 1 --is_3Di 1 --per_protein 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "45b4f0c7-c818-4131-99b0-971d11e804c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de proteínas en el archivo: 18256\n"
     ]
    }
   ],
   "source": [
    "# Saber el total embeddigs generados en el conjunto de datos\n",
    "with h5py.File('ProstT5_output/structure_embeddings_train.h5', 'r') as f:\n",
    "    total_proteins = len(f.keys())\n",
    "    print(f\"Total de proteínas en el archivo: {total_proteins}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c01b35a9-d4c8-4e10-b2d0-ea6e488825a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A0A023PXQ4: shape (1024,)\n",
      "A0A023T778: shape (1024,)\n",
      "A0A061ACF5: shape (1024,)\n",
      "A0A061ACH8: shape (1024,)\n",
      "A0A061ACH9: shape (1024,)\n",
      "A0A061ACL6: shape (1024,)\n",
      "A0A061ACM7: shape (1024,)\n",
      "A0A061ACQ8: shape (1024,)\n",
      "A0A061ACX4: shape (1024,)\n",
      "A0A061AD29: shape (1024,)\n"
     ]
    }
   ],
   "source": [
    "# Revisar la dimensión de los embeddings\n",
    "with h5py.File('ProstT5_output/structure_embeddings_train.h5', 'r') as f:\n",
    "    # Iterar solo sobre los primeros 10 IDs de proteínas\n",
    "    for protein_id in islice(f.keys(), 10):\n",
    "        # Acceder al embedding\n",
    "        embedding = f[protein_id][:]\n",
    "        print(f\"{protein_id}: shape {embedding.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c9a67f7a-8337-4a89-9466-5c4cbaeee6a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A0A023PXQ4: type <class 'numpy.ndarray'>\n",
      "A0A023T778: type <class 'numpy.ndarray'>\n",
      "A0A061ACF5: type <class 'numpy.ndarray'>\n",
      "A0A061ACH8: type <class 'numpy.ndarray'>\n",
      "A0A061ACH9: type <class 'numpy.ndarray'>\n",
      "A0A061ACL6: type <class 'numpy.ndarray'>\n",
      "A0A061ACM7: type <class 'numpy.ndarray'>\n",
      "A0A061ACQ8: type <class 'numpy.ndarray'>\n",
      "A0A061ACX4: type <class 'numpy.ndarray'>\n",
      "A0A061AD29: type <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "# Verificar el tipo de dato de los embeddings\n",
    "with h5py.File('ProstT5_output/structure_embeddings_train.h5', 'r') as f:\n",
    "    # Iterar solo sobre los primeros 10 IDs de proteínas\n",
    "    for protein_id in islice(f.keys(), 10):\n",
    "        # Acceder al embedding\n",
    "        embedding = f[protein_id][:]\n",
    "        print(f\"{protein_id}: type {type(embedding)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875c7bcf",
   "metadata": {},
   "source": [
    "### Conjunto de datos de validación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93b91b14-742e-4223-8403-613501b7743f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-04 13:00:31.363521: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-05-04 13:00:31.386079: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-05-04 13:00:31.717992: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Using device: cuda:0\n",
      "Loading T5 from: Rostlab/ProstT5\n",
      "/home/sis.virtual.uniandes.edu.co/as.villa/miniconda3/envs/bioembed/lib/python3.8/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Using model in half-precision!\n",
      "########################################\n",
      "Input is 3Di: True\n",
      "Example sequence: A0A087WRJ2\n",
      "dqdkdkdwdwfqfpnaikikikiagdppaaapaeeeeedaqvdfnvvcvvvcvrvvcrvvrhiymtigadcddvnnvgdqqddqqaahalvrvvrvcvrvvrpayeyeaeasclshvvnvqldvprrhpyyhyhpyhppvvddpvsvvpgd\n",
      "########################################\n",
      "Total number of sequences: 513\n",
      "Average sequence length: 420.40155945419104\n",
      "Number of sequences >1000: 26\n",
      "Example: embedded protein P35194 with length 2493 to emb. of shape: torch.Size([1024])\n",
      "\n",
      "############# STATS #############\n",
      "Total number of embeddings: 513\n",
      "Total time: 10.66[s]; time/prot: 0.0208[s]; avg. len= 420.40\n"
     ]
    }
   ],
   "source": [
    "!python ProstT5-main/scripts/embed.py --input 3di_data/val_3di.fasta --output ProstT5_output/structure_embeddings_val.h5 --half 1 --is_3Di 1 --per_protein 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0eb71704-f2ed-424f-90a6-6a40fbf9aa8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de proteínas en el archivo: 513\n"
     ]
    }
   ],
   "source": [
    "# Saber el total embeddigs generados en el conjunto de datos \n",
    "import h5py\n",
    "from itertools import islice\n",
    "with h5py.File('structure_embeddings/structure_embeddings_val.h5', 'r') as f:\n",
    "    total_proteins = len(f.keys())\n",
    "    print(f\"Total de proteínas en el archivo: {total_proteins}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "63527883-70e1-49cc-864a-21a7f9d25ab0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A0A087WRJ2: shape (1024,)\n",
      "A0A0B5JS55: shape (1024,)\n",
      "A0A0G2JE97: shape (1024,)\n",
      "A0A0G2KCY3: shape (1024,)\n",
      "A0A0G2L325: shape (1024,)\n",
      "A0A0K2H545: shape (1024,)\n",
      "A0A0R4IBM8: shape (1024,)\n",
      "A0A0R4IEZ3: shape (1024,)\n",
      "A0A0R4IKF5: shape (1024,)\n",
      "A0A0R4IP63: shape (1024,)\n"
     ]
    }
   ],
   "source": [
    "# Revisar la dimensión de los embeddings\n",
    "with h5py.File('ProstT5_output/structure_embeddings_val.h5', 'r') as f:\n",
    "    # Iterar solo sobre los primeros 10 IDs de proteínas\n",
    "    for protein_id in islice(f.keys(), 10):\n",
    "        # Acceder al embedding\n",
    "        embedding = f[protein_id][:]\n",
    "        print(f\"{protein_id}: shape {embedding.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b5e8f6fa-3e9f-4bd2-9fcd-6215e9309f65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A0A087WRJ2: type <class 'numpy.ndarray'>\n",
      "A0A0B5JS55: type <class 'numpy.ndarray'>\n",
      "A0A0G2JE97: type <class 'numpy.ndarray'>\n",
      "A0A0G2KCY3: type <class 'numpy.ndarray'>\n",
      "A0A0G2L325: type <class 'numpy.ndarray'>\n",
      "A0A0K2H545: type <class 'numpy.ndarray'>\n",
      "A0A0R4IBM8: type <class 'numpy.ndarray'>\n",
      "A0A0R4IEZ3: type <class 'numpy.ndarray'>\n",
      "A0A0R4IKF5: type <class 'numpy.ndarray'>\n",
      "A0A0R4IP63: type <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "# Verificar el tipo de dato de los embeddings\n",
    "with h5py.File('ProstT5_output/structure_embeddings_val.h5', 'r') as f:\n",
    "    # Iterar solo sobre los primeros 10 IDs de proteínas\n",
    "    for protein_id in islice(f.keys(), 10):\n",
    "        # Acceder al embedding\n",
    "        embedding = f[protein_id][:]\n",
    "        print(f\"{protein_id}: type {type(embedding)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e42816",
   "metadata": {},
   "source": [
    "### Conjunto de datos de evaluación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ba73d22-c4f7-4e21-b587-629a773f4ccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-04 13:04:28.744354: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-05-04 13:04:28.766886: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-05-04 13:04:29.097794: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Using device: cuda:0\n",
      "Loading T5 from: Rostlab/ProstT5\n",
      "/home/sis.virtual.uniandes.edu.co/as.villa/miniconda3/envs/bioembed/lib/python3.8/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Using model in half-precision!\n",
      "########################################\n",
      "Input is 3Di: True\n",
      "Example sequence: A0A0K2H416\n",
      "ddddpfllvqqapfdkdwddfqfdpffeteiegqqcsrslgfplllllqllvvvcvvvvladapaaaeeeqdldsnlssnlqnclrrrhayeyeaapprdpvsvvssvvsvhhydhfhcvchsvrsvvvsvvvpvvrpsydyspllahlsllqsllpgvlvsvcvvcvlqaqeeffeddsqsnvqnnlvnscvsnvnyayeyedelqqpvlvpdaggdadqprhrpryhrpsrdnvrhdyydhdyrvqllvqqvcccvrvvaaagsrqsrrvsvqsvvrrvhgyphyyyyyggggnvvdpvdpsppd\n",
      "########################################\n",
      "Total number of sequences: 523\n",
      "Average sequence length: 423.2963671128107\n",
      "Number of sequences >1000: 29\n",
      "Example: embedded protein Q5D862 with length 2391 to emb. of shape: torch.Size([1024])\n",
      "\n",
      "############# STATS #############\n",
      "Total number of embeddings: 523\n",
      "Total time: 10.84[s]; time/prot: 0.0207[s]; avg. len= 423.30\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!python ProstT5-main/scripts/embed.py --input 3di_data/test_3di.fasta --output ProstT5_output/structure_embeddings_test.h5 --half 1 --is_3Di 1 --per_protein 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "97242797-4dd6-4450-b0c2-b31b976dbd7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de proteínas en el archivo: 523\n"
     ]
    }
   ],
   "source": [
    "# Saber el total embeddigs generados en el conjunto de datos \n",
    "with h5py.File('ProstT5_output/structure_embeddings_test.h5', 'r') as f:\n",
    "    total_proteins = len(f.keys())\n",
    "    print(f\"Total de proteínas en el archivo: {total_proteins}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9f23a559-725f-4099-b40a-10adb3460210",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A0A0K2H416: shape (1024,)\n",
      "A0A0K2H4T3: shape (1024,)\n",
      "A0A0K2H4Y0: shape (1024,)\n",
      "A0A0K2H571: shape (1024,)\n",
      "A0A0K2H597: shape (1024,)\n",
      "A0A0K2H599: shape (1024,)\n",
      "A0A0K2H5Z1: shape (1024,)\n",
      "A0A0K2H6J8: shape (1024,)\n",
      "A0A0K2H6X7: shape (1024,)\n",
      "A0A0K2H776: shape (1024,)\n"
     ]
    }
   ],
   "source": [
    "# Revisar la dimensión de los embeddings\n",
    "with h5py.File('ProstT5_output/structure_embeddings_test.h5', 'r') as f:\n",
    "    # Iterar solo sobre los primeros 10 IDs de proteínas\n",
    "    for protein_id in islice(f.keys(), 10):\n",
    "        # Acceder al embedding\n",
    "        embedding = f[protein_id][:]\n",
    "        print(f\"{protein_id}: shape {embedding.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "37924bb6-a61c-47d5-a4a3-138a6a27d0b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A0A0K2H416: type <class 'numpy.ndarray'>\n",
      "A0A0K2H4T3: type <class 'numpy.ndarray'>\n",
      "A0A0K2H4Y0: type <class 'numpy.ndarray'>\n",
      "A0A0K2H571: type <class 'numpy.ndarray'>\n",
      "A0A0K2H597: type <class 'numpy.ndarray'>\n",
      "A0A0K2H599: type <class 'numpy.ndarray'>\n",
      "A0A0K2H5Z1: type <class 'numpy.ndarray'>\n",
      "A0A0K2H6J8: type <class 'numpy.ndarray'>\n",
      "A0A0K2H6X7: type <class 'numpy.ndarray'>\n",
      "A0A0K2H776: type <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "# Verificar el tipo de dato de los embeddings\n",
    "with h5py.File('ProstT5_output/structure_embeddings_test.h5', 'r') as f:\n",
    "    # Iterar solo sobre los primeros 10 IDs de proteínas\n",
    "    for protein_id in islice(f.keys(), 10):\n",
    "        # Acceder al embedding\n",
    "        embedding = f[protein_id][:]\n",
    "        print(f\"{protein_id}: type {type(embedding)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
